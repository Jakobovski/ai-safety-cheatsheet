# AI Safety Cheatsheet
This is intended to be a compilation of the big AI safety ideas, problems and approaches to solutions. In order to keep it readable, we provide links to the content instead of the content itself. **Contributions are welcome!**

## Alignment Problems

### Forecasting
* More is different [[1]](https://bounded-regret.ghost.io/more-is-different-for-ai/) [[2]](https://www.science.org/doi/10.1126/science.177.4047.393) [[3]](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/)
* Vinge uncertainty [[1]](https://arbital.com/p/Vingean_uncertainty/) [[2]](https://arbital.com/p/Vinge_principle/) [[3]](https://arbital.com/p/Vinge_law/)
* Collingridge dilemma [[1]](https://en.wikipedia.org/wiki/Collingridge_dilemma)
* Bio-anchors [[1]](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/)
* Thought experiments [[1]](https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/)

### Optimization issues
* Inner alignment.   [[1]](https://arxiv.org/abs/2105.14111) [[2]](https://aligned.substack.com/p/inner-alignment)
  * Out of distribution alignment / goal misgeneralization  [[1]](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) [[2]](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization)  [[3]](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)
* Outer alignment and Mesa-optimizers  [[1]](https://arxiv.org/abs/1906.01820) [[2]](https://www.youtube.com/watch?v=bJLcIBixGj8) [[3]](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)

### Human-like behaviors
* Instrumental convergence [[1]](https://en.wikipedia.org/wiki/Instrumental_convergence) [[2]](https://www.lesswrong.com/tag/instrumental-convergence) [[3]](https://arbital.com/p/instrumental_convergence/) [[4]](https://nickbostrom.com/superintelligentwill.pdf)
    * Self-preservation
    * Goal-content integrity
    * Cognitive enhancement
    * Resource acquisition
    * Power/Influence acquisition [[1]](https://neurips.cc/virtual/2021/poster/28400)
* Specification gaming [[1]](https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4) [[2]](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtmlcc)
  * Goodharts law [[1]](https://en.wikipedia.org/wiki/Goodhart%27s_law) and Goodharts Curse [[2]](https://arbital.com/p/goodharts_curse/)
* Deception. This is the optimal behavior for a misaligned mesa-optimizer. [[1]](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
* Nearest unblocked strategy [[1]](https://arbital.com/p/nearest_unblocked/)
* Collaboration with other AIs
* Sycophant AI [[1]](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)


### Alien behaviors
* Orthogonality Thesis [[1]](https://www.lesswrong.com/tag/orthogonality-thesis) [[2]](https://arbital.com/p/orthogonality/) [[3]](https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf) [[4]](https://nickbostrom.com/superintelligentwill.pdf)
* Strawberry problem [[1]](https://forum.effectivealtruism.org/posts/v2KL4ApqrxuYqQckK/ngo-and-yudkowsky-on-ai-capability-gains) [[2]](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)
* Paperclip maximizing [[1]](https://www.lesswrong.com/tag/paperclip-maximizer) [[2]](https://nickbostrom.com/ethics/ai)
* Learning the wrong distribution. [[1]](https://arxiv.org/pdf/1905.02175.pdf)  [[2]](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
* High impact [[1]](https://arbital.com/p/low_impact/)
* Edge instantiation [[1]](https://arbital.com/p/edge_instantiation/)
* Context disaster [[1]](https://arbital.com/p/context_disaster/)

## Deployment Problems
* Alignment tax / Safety Tax [[1]](https://forum.effectivealtruism.org/topics/alignment-tax)
* Collingridge dilemma [[1]](https://en.wikipedia.org/wiki/Collingridge_dilemma)
* Corrigibility [[1]](https://intelligence.org/files/Corrigibility.pdf) [[2]](https://arbital.com/p/corrigibility/)
* Humans are not secure [[1]](https://en.wikipedia.org/wiki/AI_box)
* AI-Box [[1]](https://en.wikipedia.org/wiki/AI_box) [[2]](https://arbital.com/p/AI_boxing/)
* We need to get alignment right on the 'first critical try' [[1]](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)
* Shutdown problem [[1]](https://arbital.com/p/shutdown_problem/)

## Governance
Overview [[1]](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf) [[2]](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact) [[3]](https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview)
* Robust totalitarianism [[1]](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf) [[2]](https://forum.effectivealtruism.org/topics/totalitarianism)
* Extreme first-strike advantages [[1]](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf) [[2]](https://www.rand.org/content/dam/rand/pubs/perspectives/PE200/PE296/RAND_PE296.pdf)
* Misuse Risks [[1]](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)
* Value Erosion through Competition  [[1]](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact) [[2]](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
* Windfall clause [[1]](https://www.fhi.ox.ac.uk/wp-content/uploads/Windfall-Clause-Report.pdf)
* Compute governance [[1]](https://forum.effectivealtruism.org/topics/compute-governance)
* Risks from malevolent actors [[1]](https://forum.effectivealtruism.org/topics/risks-from-malevolent-actors)


## Models for thinking about AGI agents
* Human-anchors [[1]](https://bounded-regret.ghost.io/p/a2d733a7-108a-4587-97fb-db90f66ce030/)
* Bio-anchors [[1]](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/)
* A super-smart deceptive, manipulative, psychopath with arbitrary and (possibly absurd) goals.
* As a computer program that simply does what it’s programmed to do. Just because it is super capable does not mean it is wise, moral, smart or cares about what humans want.

## Approaches to AI safety
* Eliciting latent knowledge (Paul Christiano) (Alignment Research Center) [[1]](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)
* Agent foundations (MIRI) [[1]](https://intelligence.org/files/TechnicalAgenda.pdf)
* Brain-like design [[1]](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)
* Iterated Distillation and Amplification [[1]](https://arxiv.org/abs/1810.08575)
* Humans Consulting Humans (Christiano) [[1]](https://ai-alignment.com/strong-hch-bedb0dc08d4e)
* Learning from Humans [[1]](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) [[2]](https://www.youtube.com/watch?v=kGc8jOy5_zY) [[3]](https://stuhlmueller.org/papers/preferences-nipsworkshop2015.pdf)
* Reward modeling (DeepMind) [[1]](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84)
* Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstration [[1]](http://proceedings.mlr.press/v100/brown20a/brown20a.pdf)
* Imitation learning [[1]](https://en.wikipedia.org/wiki/Apprenticeship_learning)
* Myopic reinforcement learning [[1]](https://www.cs.utexas.edu/~bradknox/TAMER.html)
* Inverse reinforcement learning [[1]](http://ftp.cs.berkeley.edu/~russell/papers/colt98-uncertainty.pdf)
* Cooperative inverse reinforcement learning [[1]](https://arxiv.org/abs/1606.03137)
* Debate [[1]](https://arxiv.org/abs/1805.00899) [[2]](https://openai.com/blog/debate/)
* Capability control method
  * Oracle AI [[1]](https://forum.effectivealtruism.org/topics/oracle-ai)
  * AI Boxing [[1]](https://forum.effectivealtruism.org/topics/ai-boxing) [[2]](https://en.wikipedia.org/wiki/AI_box) [[3]](https://arbital.com/p/AI_boxing/)
  * Anthropormorphic capture [[1]](https://forum.effectivealtruism.org/topics/anthropic-capture)
  * Stunting [[1]](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112)
  * Tripwires [[1]](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112)
* Transparency / Interpretability
  * Understandability principle [[1]](https://arbital.com/p/understandability_principle/)
  * Effability [[2]](https://arbital.com/p/effability/)

## Definitions

#### General Intelligence
* "General Intelligence or Universal Intelligence is the ability to efficiently achieve goals in a wide range of domains". (This is a commonly held definition) [[1]](https://www.lesswrong.com/tag/general-intelligence) [[2]](https://arbital.com/p/general_intelligence/)
* "Intelligence is the ability to make models. General intelligence means that a sufficiently large computational substrate can be fitted to an arbitrary computable function, within the limits of that substrate." (Josha Bach) [[1]](https://twitter.com/plinz/status/1019328120497803271)

#### Alignment
* "AI that is trying to do what you want it to do". (Paul Christiano) [[1](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)]
* "AI systems be designed with the sole objective of maximizing the realization of human preferences" (Stuart Russell) [[1]](https://en.wikipedia.org/wiki/AI_alignment)
* "AI should be designed to align with our ‘coherent extrapolated volition’ (CEV)[[1]](https://arbital.com/p/cev/). CEV represents an integrated version of what we would want ‘if we knew more, thought faster, were more the people we wished we were, and had grown up farther together" (Eliezer Yudkowsky) [[1]](https://link.springer.com/article/10.1007/s11023-020-09539-2)



## Meta Resources
* [2022 AGI Safety Fundamentals alignment curriculum](https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/)
* [AI Safety Syllabus](https://80000hours.org/articles/ai-safety-syllabus/)
* [Awesome AI Safety](https://github.com/hari-sikchi/awesome-ai-safety)
* [Awesome AI Alignment](https://bytemeta.vip/repo/dit7ya/awesome-ai-alignment)
* [AI Alignment resources arbital](https://arbital.com/explore/ai_alignment/)
* [AGISafety.org](http://agisafety.org)

## About Cheatsheet
Contributions are welcome! Please open a merge request and will do my best to quickly approve it.